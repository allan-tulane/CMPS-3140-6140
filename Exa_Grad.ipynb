{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ-huZK658h8",
        "outputId": "0b2ed48f-202b-4f1b-8669-3613c4e23470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x780f547348b0>\n",
            "Gradient for w = tensor([[-0.0731,  0.0027, -0.0074],\n",
            "        [-0.0731,  0.0027, -0.0074],\n",
            "        [-0.0731,  0.0027, -0.0074],\n",
            "        [-0.0731,  0.0027, -0.0074],\n",
            "        [-0.0731,  0.0027, -0.0074]])\n",
            "Gradient for b = tensor([-0.0731,  0.0027, -0.0074])\n",
            "Target used: tensor([1., 0., 1.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 1. Create tensors that require gradient calculation (leaf nodes)\n",
        "x = torch.ones(5)  # input tensor\n",
        "w = torch.randn(5, 3, requires_grad=True)  # weight parameter\n",
        "b = torch.randn(3, requires_grad=True)     # bias parameter\n",
        "\n",
        "# Create a target variable (random binary labels for demonstration)\n",
        "target = torch.randint(0, 2, (3,)).float()  # random 0/1 values\n",
        "\n",
        "# 2. Perform forward propagation, PyTorch dynamically builds the computation graph\n",
        "z = torch.matmul(x, w) + b  # linear transformation\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, target)\n",
        "\n",
        "# Check the grad_fn attribute of loss, which points to a loss function object\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")\n",
        "\n",
        "# 3. Perform backpropagation, calculate gradients\n",
        "loss.backward()\n",
        "\n",
        "# 4. View the calculated gradients\n",
        "print(f\"Gradient for w = {w.grad}\")\n",
        "print(f\"Gradient for b = {b.grad}\")\n",
        "print(f\"Target used: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. Create tensors that require gradient calculation (leaf nodes)\n",
        "x = torch.ones(5)  # input tensor\n",
        "\n",
        "# First layer parameters (5 → 4 hidden units)\n",
        "w1 = torch.randn(5, 4, requires_grad=True)\n",
        "b1 = torch.randn(4, requires_grad=True)\n",
        "\n",
        "# Second layer parameters (4 → 3 output units)\n",
        "w2 = torch.randn(4, 3, requires_grad=True)\n",
        "b2 = torch.randn(3, requires_grad=True)\n",
        "\n",
        "# Target variable\n",
        "target = torch.randint(0, 2, (3,)).float()\n",
        "\n",
        "# 2. Forward propagation (two layers with sigmoid activation)\n",
        "\n",
        "# Layer 1: linear\n",
        "z1 = torch.matmul(x, w1) + b1\n",
        "\n",
        "# Sigmoid activation\n",
        "a1 = torch.sigmoid(z1)\n",
        "\n",
        "# Layer 2: linear\n",
        "z2 = torch.matmul(a1, w2) + b2\n",
        "\n",
        "# Loss (BCE with logits expects raw logits)\n",
        "loss = F.binary_cross_entropy_with_logits(z2, target)\n",
        "\n",
        "# Check grad_fn\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")\n",
        "\n",
        "# 3. Backpropagation\n",
        "loss.backward()\n",
        "\n",
        "# 4. View gradients\n",
        "print(f\"Gradient for w1 = {w1.grad}\")\n",
        "print(f\"Gradient for b1 = {b1.grad}\")\n",
        "print(f\"Gradient for w2 = {w2.grad}\")\n",
        "print(f\"Gradient for b2 = {b2.grad}\")\n",
        "print(f\"Target used: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZfXq3Y2-OGQ",
        "outputId": "32cdbc34-3e99-4654-fd1a-14ff1f56df86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7d2fe0fb9690>\n",
            "Gradient for w1 = tensor([[-0.0318,  0.0358,  0.0994, -0.0200],\n",
            "        [-0.0318,  0.0358,  0.0994, -0.0200],\n",
            "        [-0.0318,  0.0358,  0.0994, -0.0200],\n",
            "        [-0.0318,  0.0358,  0.0994, -0.0200],\n",
            "        [-0.0318,  0.0358,  0.0994, -0.0200]])\n",
            "Gradient for b1 = tensor([-0.0318,  0.0358,  0.0994, -0.0200])\n",
            "Gradient for w2 = tensor([[-0.1261, -0.2682, -0.1226],\n",
            "        [-0.0098, -0.0208, -0.0095],\n",
            "        [-0.0320, -0.0682, -0.0311],\n",
            "        [-0.0099, -0.0211, -0.0096]])\n",
            "Gradient for b2 = tensor([-0.1497, -0.3184, -0.1455])\n",
            "Target used: tensor([1., 1., 1.])\n"
          ]
        }
      ]
    }
  ]
}