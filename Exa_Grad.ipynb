{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ-huZK658h8",
        "outputId": "0b2ed48f-202b-4f1b-8669-3613c4e23470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x780f547348b0>\n",
            "Gradient for w = tensor([[-0.0731,  0.0027, -0.0074],\n",
            "        [-0.0731,  0.0027, -0.0074],\n",
            "        [-0.0731,  0.0027, -0.0074],\n",
            "        [-0.0731,  0.0027, -0.0074],\n",
            "        [-0.0731,  0.0027, -0.0074]])\n",
            "Gradient for b = tensor([-0.0731,  0.0027, -0.0074])\n",
            "Target used: tensor([1., 0., 1.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 1. Create tensors that require gradient calculation (leaf nodes)\n",
        "x = torch.ones(5)  # input tensor\n",
        "w = torch.randn(5, 3, requires_grad=True)  # weight parameter\n",
        "b = torch.randn(3, requires_grad=True)     # bias parameter\n",
        "\n",
        "# Create a target variable (random binary labels for demonstration)\n",
        "target = torch.randint(0, 2, (3,)).float()  # random 0/1 values\n",
        "\n",
        "# 2. Perform forward propagation, PyTorch dynamically builds the computation graph\n",
        "z = torch.matmul(x, w) + b  # linear transformation\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, target)\n",
        "\n",
        "# Check the grad_fn attribute of loss, which points to a loss function object\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")\n",
        "\n",
        "# 3. Perform backpropagation, calculate gradients\n",
        "loss.backward()\n",
        "\n",
        "# 4. View the calculated gradients\n",
        "print(f\"Gradient for w = {w.grad}\")\n",
        "print(f\"Gradient for b = {b.grad}\")\n",
        "print(f\"Target used: {target}\")"
      ]
    }
  ]
}